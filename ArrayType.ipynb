{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e25e7bd-727c-4112-b872-90fac453b379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n|name|numbers|\n+----+-------+\n|hary| [2, 3]|\n|paul| [5, 6]|\n+----+-------+\n\nroot\n |-- name: string (nullable = true)\n |-- numbers: array (nullable = true)\n |    |-- element: long (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [('hary', [2,3]), ('paul', [5,6])]\n",
    "schema = ('name', 'numbers')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04063b9-509c-45b6-bd61-810ac1e27eb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n|name|number|\n+----+------+\n|hary|[2, 3]|\n|paul|[5, 6]|\n+----+------+\n\nroot\n |-- name: string (nullable = true)\n |-- number: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "data = [('hary', [2,3]), ('paul', [5,6])]\n",
    "schema = StructType([\\\n",
    "                    StructField('name', StringType()),\\\n",
    "                    StructField('number', ArrayType(IntegerType()))\\\n",
    "                    ])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcea0c3-38de-4de5-9706-3658386cb504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+\n|name|number|first_number|\n+----+------+------------+\n|hary|[2, 3]|           2|\n|paul|[5, 6]|           5|\n+----+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn('first_number', col('number')[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44b46a9-5772-40e0-b43d-426867dba89b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n|name|number|firstnumber|\n+----+------+-----------+\n|hary|[2, 3]|          2|\n|paul|[5, 6]|          5|\n+----+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn('firstnumber', col('number')[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accb7f9d-a7aa-44e0-a898-ebf84f6254f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|num1|num2|\n+----+----+\n|   1|   2|\n|   3|   4|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,2),(3,4)]\n",
    "schema = ('num1', 'num2')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b0e27d-1b2b-46a6-98ac-0acae47113ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n|num1|num2|numbers|\n+----+----+-------+\n|   1|   2| [1, 2]|\n|   3|   4| [3, 4]|\n+----+----+-------+\n\nroot\n |-- num1: long (nullable = true)\n |-- num2: long (nullable = true)\n |-- numbers: array (nullable = false)\n |    |-- element: long (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "\n",
    "df1 = df.withColumn('numbers', array(col('num1'), col('num2')))\n",
    "\n",
    "df1.show()\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c741582b-afd2-4642-b4f5-e844bc2e63d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n| id|  name|       skills|\n+---+------+-------------+\n|  1|pavani|[aws, python]|\n|  2|  john| [azure, sql]|\n+---+------+-------------+\n\n+---+------+-------------+------+\n| id|  name|       skills| skill|\n+---+------+-------------+------+\n|  1|pavani|[aws, python]|   aws|\n|  1|pavani|[aws, python]|python|\n|  2|  john| [azure, sql]| azure|\n|  2|  john| [azure, sql]|   sql|\n+---+------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'pavani', ['aws', 'python']), (2,'john', ['azure', 'sql'])]\n",
    "schema = ('id', 'name', 'skills')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df1 = df.withColumn('skill', explode(col('skills')))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2fab6b-1acf-4e1b-8089-771c7db1f0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------+\n| id|  name|     skills|   skillsArray|\n+---+------+-----------+--------------+\n|  1|pavani|aws, python|[aws,  python]|\n|  2|  john|  azure,sql|  [azure, sql]|\n+---+------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "data = [(1,'pavani', 'aws, python'), (2,'john', 'azure,sql')]\n",
    "schema = ('id', 'name', 'skills')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df1 = df.withColumn('skillsArray', split(col('skills'), ','))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9d7fd3-ec4e-4202-9f26-d73d86a4692c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+---------------+\n| id|  name|primary skill|secondary skill|\n+---+------+-------------+---------------+\n|  1|pavani|          aws|         python|\n|  2|  john|        azure|            sql|\n+---+------+-------------+---------------+\n\n+---+------+-------------+---------------+-------------+\n| id|  name|primary skill|secondary skill|  skillsArray|\n+---+------+-------------+---------------+-------------+\n|  1|pavani|          aws|         python|[aws, python]|\n|  2|  john|        azure|            sql| [azure, sql]|\n+---+------+-------------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "data = [(1,'pavani', 'aws', 'python'), (2,'john', 'azure', 'sql')]\n",
    "schema = ('id', 'name', 'primary skill', 'secondary skill')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df1 = df.withColumn('skillsArray', array(col('primary skill'), col('secondary skill')))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183652cc-3857-44cd-b2e2-6b29add41d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n| id|  name|       skills|\n+---+------+-------------+\n|  1|pavani|[aws, python]|\n|  2|  john| [azure, sql]|\n+---+------+-------------+\n\n+---+------+-------------+-------+\n| id|  name|       skills|has_aws|\n+---+------+-------------+-------+\n|  1|pavani|[aws, python]|   true|\n|  2|  john| [azure, sql]|  false|\n+---+------+-------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "data = [(1,'pavani', ['aws', 'python']), (2,'john', ['azure', 'sql'])]\n",
    "schema = ('id', 'name', 'skills')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df1 = df.withColumn('has_aws', array_contains(col('skills'), 'aws'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad72d0c-cf80-4909-89ad-6d7f387eff6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|  name|         properities|\n+------+--------------------+\n|maheer|{eye -> black, ha...|\n|lokesh|{eye -> brown, ha...|\n+------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properities: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', {'hair': 'brown', 'eye': 'black'}), ('lokesh', {'hair': 'black', 'eye': 'brown'}) ]\n",
    "schema = ('name', 'properities')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f7a40a-1176-41e8-ae64-c2ccaaed1086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|  name|         properities|\n+------+--------------------+\n|maheer|{eye -> black, ha...|\n|lokesh|{eye -> brown, ha...|\n+------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properities: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StringType, MapType\n",
    "\n",
    "data = [('maheer', {'hair': 'brown', 'eye': 'black'}), ('lokesh', {'hair': 'black', 'eye': 'brown'}) ]\n",
    "schema = StructType([StructField('name',StringType()),\\\n",
    "                     StructField('properities',  MapType(StringType(), StringType()))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1157e289-73cd-46bb-b625-42d77183c511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+\n|  name|         properities|  eye|\n+------+--------------------+-----+\n|maheer|{eye -> black, ha...|black|\n|lokesh|{eye -> brown, ha...|brown|\n+------+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('eye', df.properities['eye'])\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c5b9c2-3118-48a1-9bc4-e67ef034b5f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+-----+\n|  name|         properities|  eye| hair|\n+------+--------------------+-----+-----+\n|maheer|{eye -> black, ha...|black|brown|\n|lokesh|{eye -> brown, ha...|brown|black|\n+------+--------------------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('hair', df.properities.getItem('hair'))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9115b72f-03b5-44a8-a122-63e57ca23d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+\n|name  |properities                  |\n+------+-----------------------------+\n|maheer|{eye -> black, hair -> brown}|\n|lokesh|{eye -> brown, hair -> black}|\n+------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properities: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+------+--------------------+----+-----+\n|  name|         properities| key|value|\n+------+--------------------+----+-----+\n|maheer|{eye -> black, ha...| eye|black|\n|maheer|{eye -> black, ha...|hair|brown|\n|lokesh|{eye -> brown, ha...| eye|brown|\n|lokesh|{eye -> brown, ha...|hair|black|\n+------+--------------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StringType, MapType\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "data = [('maheer', {'hair': 'brown', 'eye': 'black'}), ('lokesh', {'hair': 'black', 'eye': 'brown'}) ]\n",
    "schema = StructType([StructField('name',StringType()),\\\n",
    "                     StructField('properities',  MapType(StringType(), StringType()))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "df1 = df.select('name', 'properities', explode(col('properities')))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c17fb9-9e30-4bb4-99af-33758b697071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------+\n|  name|         properities|       keys|\n+------+--------------------+-----------+\n|maheer|{eye -> black, ha...|[eye, hair]|\n|lokesh|{eye -> brown, ha...|[eye, hair]|\n+------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df1 = df.withColumn('keys', map_keys('properities'))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f122106-b478-4a68-8196-5d259204033e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+\n|  name|         properities|        values|\n+------+--------------------+--------------+\n|maheer|{eye -> black, ha...|[black, brown]|\n|lokesh|{eye -> brown, ha...|[brown, black]|\n+------+--------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df1 = df.withColumn('values', map_values('properities'))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e34d9f-2a2b-44bb-8ef8-cf685c3f6c4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ArrayType",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
