{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd258dfe-69f4-4d88-b187-5677840bd481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4401708495194438#setting/sparkui/0415-201755-srclnw15/driver-4727120441602748485\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4401708495194438#setting/sparkui/0415-201755-srclnw15/driver-4727120441602748485\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4303332e-e368-4f3e-afbd-837bf724b0cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: pyspark.sql.session.SparkSession"
     ]
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774ab302-0b08-4de4-9044-327f86a37ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['Builder',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_activeSession',\n '_conf',\n '_convert_from_pandas',\n '_createFromLocal',\n '_createFromLocalTrusted',\n '_createFromRDD',\n '_create_dataframe',\n '_create_from_pandas_with_arrow',\n '_create_rdd_from_local_trusted',\n '_create_shell_session',\n '_getActiveSessionOrCreate',\n '_get_numpy_record_dtype',\n '_inferSchema',\n '_inferSchemaFromList',\n '_instantiatedSession',\n '_jconf',\n '_jsc',\n '_jsparkSession',\n '_jvm',\n '_repr_html_',\n '_sc',\n '_wrap_data_schema',\n '_write_to_trusted_path',\n 'builder',\n 'catalog',\n 'conf',\n 'createDataFrame',\n 'getActiveSession',\n 'newSession',\n 'range',\n 'read',\n 'readStream',\n 'sparkContext',\n 'sql',\n 'stop',\n 'streams',\n 'table',\n 'udf',\n 'version']"
     ]
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e33ca6-0e50-4e28-b094-51a62a0f5d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a list of dictionaries\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    Create a DataFrame from an RDD.\n    \n    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from Row instances.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    \n    When the type is unmatched, it throws an exception.\n    \n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3259d6a5-e0f9-4857-a583-59d7a5088b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| Id|  Name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'Maheer'),(2, 'wafa')]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = ['Id','Name'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06db6231-4b2a-4b8c-9fdd-5047341aa917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| Id|  Name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1,'Maheer'),(2, 'wafa')]\n",
    "\n",
    "schema = StructType([StructField(name = 'Id', dataType = IntegerType()),\n",
    "            StructField(name = 'Name', dataType = StringType())])\n",
    "\n",
    "type(schema)\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff8569d-2f43-4d81-b97d-20da5956cbf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [{'id':1,'name':'Maheer'},\n",
    "        {'id':2,'name':'wafa'}]\n",
    "\n",
    "df = spark.createDataFrame(data=data)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04faac87-5cae-438d-90b1-b9b2009950e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n|Age|Premium|\n+---+-------+\n| 18|  10000|\n| 22|  15000|\n| 23|  18000|\n| 26|  21000|\n| 28|  24000|\n| 31|  26500|\n| 33|  27000|\n+---+-------+\n\nroot\n |-- Age: string (nullable = true)\n |-- Premium: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "## Reading csv file\n",
    "\n",
    "df = spark.read.csv(path = 'dbfs:/FileStore/tables/simplelinearregression-2.csv', header=True,)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56efa2a0-06de-436e-a19b-4a7d64c8f6cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n|Age|Premium|\n+---+-------+\n| 18|  10000|\n| 22|  15000|\n| 23|  18000|\n| 26|  21000|\n| 28|  24000|\n| 31|  26500|\n| 33|  27000|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option(key=\"header\", value=True).load(path = 'dbfs:/FileStore/tables/simplelinearregression-2.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e133aa-8f09-4677-9d14-980d0e9fdf30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n|Age|Premium|\n+---+-------+\n| 18|  10000|\n| 22|  15000|\n| 23|  18000|\n| 26|  21000|\n| 28|  24000|\n| 31|  26500|\n| 33|  27000|\n| 18|  10000|\n| 22|  15000|\n| 23|  18000|\n| 26|  21000|\n| 28|  24000|\n| 31|  26500|\n| 33|  27000|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#read multiple csv files from different paths/folders\n",
    "\n",
    "df1 = spark.read.csv(path = [\"dbfs:/FileStore/tables/simplelinearregression-1.csv\", \"dbfs:/FileStore/tables/simplelinearregression-2.csv\"], header=True)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e0075f-cb24-4a2e-a891-342a5e025eba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read all csv files in a folder\n",
    "\n",
    "df1 = spark.read.csv(path = \"dbfs:/FileStore/tables/\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6b6cea-e65b-445b-a916-5fe3f91e3eac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------+-------+----+-----------+----+-----+--------------+--------------------+------------+------------------+------------+\n|                 App|      Category|Rating|Reviews|Size|   Installs|Type|Price|Content Rating|              Genres|Last Updated|       Current Ver| Android Ver|\n+--------------------+--------------+------+-------+----+-----------+----+-----+--------------+--------------------+------------+------------------+------------+\n|Photo Editor & Ca...|ART_AND_DESIGN|   4.1|    159| 19M|    10,000+|Free|    0|      Everyone|        Art & Design|    7-Jan-18|             1.0.0|4.0.3 and up|\n| Coloring book moana|ART_AND_DESIGN|   3.9|    967| 14M|   500,000+|Free|    0|      Everyone|Art & Design;Pret...|   15-Jan-18|             2.0.0|4.0.3 and up|\n|U Launcher Lite â€“...|ART_AND_DESIGN|   4.7|  87510|8.7M| 5,000,000+|Free|    0|      Everyone|        Art & Design|    1-Aug-18|             1.2.4|4.0.3 and up|\n|Sketch - Draw & P...|ART_AND_DESIGN|   4.5| 215644| 25M|50,000,000+|Free|    0|          Teen|        Art & Design|    8-Jun-18|Varies with device|  4.2 and up|\n|Pixel Draw - Numb...|ART_AND_DESIGN|   4.3|    967|2.8M|   100,000+|Free|    0|      Everyone|Art & Design;Crea...|   20-Jun-18|               1.1|  4.4 and up|\n|Paper flowers ins...|ART_AND_DESIGN|   4.4|    167|5.6M|    50,000+|Free|    0|      Everyone|        Art & Design|   26-Mar-17|                 1|  2.3 and up|\n|Smoke Effect Phot...|ART_AND_DESIGN|   3.8|    178| 19M|    50,000+|Free|    0|      Everyone|        Art & Design|   26-Apr-18|               1.1|4.0.3 and up|\n|    Infinite Painter|ART_AND_DESIGN|   4.1|  36815| 29M| 1,000,000+|Free|    0|      Everyone|        Art & Design|   14-Jun-18|          6.1.61.1|  4.2 and up|\n|Garden Coloring Book|ART_AND_DESIGN|   4.4|  13791| 33M| 1,000,000+|Free|    0|      Everyone|        Art & Design|   20-Sep-17|             2.9.2|  3.0 and up|\n|Kids Paint Free -...|ART_AND_DESIGN|   4.7|    121|3.1M|    10,000+|Free|    0|      Everyone|Art & Design;Crea...|    3-Jul-18|               2.8|4.0.3 and up|\n|Text on Photo - F...|ART_AND_DESIGN|   4.4|  13880| 28M| 1,000,000+|Free|    0|      Everyone|        Art & Design|   27-Oct-17|             1.0.4|  4.1 and up|\n|Name Art Photo Ed...|ART_AND_DESIGN|   4.4|   8788| 12M| 1,000,000+|Free|    0|      Everyone|        Art & Design|   31-Jul-18|            1.0.15|  4.0 and up|\n|Tattoo Name On My...|ART_AND_DESIGN|   4.2|  44829| 20M|10,000,000+|Free|    0|          Teen|        Art & Design|    2-Apr-18|               3.8|  4.1 and up|\n|Mandala Coloring ...|ART_AND_DESIGN|   4.6|   4326| 21M|   100,000+|Free|    0|      Everyone|        Art & Design|   26-Jun-18|             1.0.4|  4.4 and up|\n|3D Color Pixel by...|ART_AND_DESIGN|   4.4|   1518| 37M|   100,000+|Free|    0|      Everyone|        Art & Design|    3-Aug-18|             1.2.3|  2.3 and up|\n|Learn To Draw Kaw...|ART_AND_DESIGN|   3.2|     55|2.7M|     5,000+|Free|    0|      Everyone|        Art & Design|    6-Jun-18|               NaN|  4.2 and up|\n|Photo Designer - ...|ART_AND_DESIGN|   4.7|   3632|5.5M|   500,000+|Free|    0|      Everyone|        Art & Design|   31-Jul-18|               3.1|  4.1 and up|\n|350 Diy Room Deco...|ART_AND_DESIGN|   4.5|     27| 17M|    10,000+|Free|    0|      Everyone|        Art & Design|    7-Nov-17|                 1|  2.3 and up|\n|FlipaClip - Carto...|ART_AND_DESIGN|   4.3| 194216| 39M| 5,000,000+|Free|    0|      Everyone|        Art & Design|    3-Aug-18|             2.2.5|4.0.3 and up|\n|        ibis Paint X|ART_AND_DESIGN|   4.6| 224399| 31M|10,000,000+|Free|    0|      Everyone|        Art & Design|   30-Jul-18|             5.5.4|  4.1 and up|\n+--------------------+--------------+------+-------+----+-----------+----+-----+--------------+--------------------+------------+------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054c1edf-6cf1-43cb-b9e5-42e16d770c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(\"Age\", IntegerType(), True).add(\"Premium\", IntegerType(), True, None)\n",
    "\n",
    "df = spark.read.csv(path = 'dbfs:/FileStore/tables/simplelinearregression-2.csv', schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9084d39e-c7f1-4db2-9111-cfa14119b46f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n| Age|Premium|\n+----+-------+\n|null|   null|\n|  18|  10000|\n|  22|  15000|\n|  23|  18000|\n|  26|  21000|\n|  28|  24000|\n|  31|  26500|\n|  33|  27000|\n+----+-------+\n\nroot\n |-- Age: integer (nullable = true)\n |-- Premium: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213f35ee-be84-4cd1-965f-c302139715bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write dataframe to csv\n",
    "\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b61499c-c145-4892-99e4-8c61b5baed12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrameWriter in module pyspark.sql.readwriter:\n\nclass DataFrameWriter(OptionUtils)\n |  DataFrameWriter(df: 'DataFrame')\n |  \n |  Interface used to write a :class:`DataFrame` to external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n |  to access this.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Method resolution order:\n |      DataFrameWriter\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: 'DataFrame')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Buckets the output by the given columns. If specified,\n |      the output is laid out on the file system similar to Hive's bucketing scheme,\n |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      numBuckets : int\n |          the number of buckets to save\n |      col : str, list or tuple\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Notes\n |      -----\n |      Applicable for file-based data sources in combination with\n |      :py:meth:`DataFrameWriter.saveAsTable`.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n |  \n |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n |              exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a CSV file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.csv(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  format(self, source: str) -> 'DataFrameWriter'\n |      Specifies the underlying output data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.format('parquet')\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format('parquet').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n |      Inserts the content of the :class:`DataFrame` to the specified table.\n |      \n |      It requires that the schema of the :class:`DataFrame` is the same as the\n |      schema of the table.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      overwrite : bool, optional\n |          If true, overwrites existing data. Disabled by default\n |      \n |      Notes\n |      -----\n |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n |      the column names and just uses position-based resolution.\n |      \n |      Examples\n |      --------\n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> df = spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... )\n |      >>> df.write.saveAsTable(\"tblA\")\n |      \n |      Insert the data into 'tblA' table but with different column names.\n |      \n |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Name of the table in the external database.\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      properties : dict\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |  \n |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n |      Saves the content of the :class:`DataFrame` in JSON format\n |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n |      specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.json(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format(\"json\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n |      Specifies the behavior when data or table already exists.\n |      \n |      Options include:\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Examples\n |      --------\n |      Raise an error when writing to an existing path.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n |      Traceback (most recent call last):\n |          ...\n |      ...AnalysisException: ...\n |      \n |      Write a Parquet file back with various options, and read it back.\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Overwrite the path with a new Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |120|Takuya Ueshin|\n |      |100| Hyukjin Kwon|\n |      +---+-------------+\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds an output option for the underlying data source.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key for the option to set.\n |      value\n |          The value for the option to set.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' with writing a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds output options for the underlying data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      **options : dict\n |          The dictionary of string keys and primitive-type values.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n |      \n |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n |      >>> schema = StructType([\n |      ...     StructField(\"age\",IntegerType(),True),\n |      ...     StructField(\"name\",StringType(),True),\n |      ... ])\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n |      ...         \"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a ORC file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a ORC file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.orc(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"orc\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.parquet(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"parquet\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n |      Partitions the output by the given columns on the file system.\n |      \n |      If specified, the output is laid out on the file system similar\n |      to Hive's partitioning scheme.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      cols : str or list\n |          name of columns\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n |      \n |      >>> import tempfile\n |      >>> import os\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).sort(\"age\").show()\n |      ...\n |      ...     # Read one partition as a DataFrame.\n |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |100| Hyukjin Kwon|\n |      |120|Ruifeng Zheng|\n |      +---+-------------+\n |      +---+\n |      |age|\n |      +---+\n |      |100|\n |      +---+\n |  \n |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the contents of the :class:`DataFrame` to a data source.\n |      \n |      The data source is specified by the ``format`` and a set of ``options``.\n |      If ``format`` is not specified, the default data source configured by\n |      ``spark.sql.sources.default`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str, optional\n |          the path in a Hadoop supported file system\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : list, optional\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format('json').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the content of the :class:`DataFrame` as the specified table.\n |      \n |      In the case the table already exists, behavior of this function depends on the\n |      save mode, specified by the `mode` function (default to throwing an exception).\n |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n |      the same as that of the existing table.\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Notes\n |      -----\n |      When `mode` is `Append`, if there is an existing table, we will use the format and\n |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n |      doesn't need to be the same as that of the existing table. Unlike\n |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n |      column names to find the correct column positions.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          the table name\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n |      partitionBy : str or list\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Creates a table from a DataFrame, and read it back.\n |      \n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.saveAsTable(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Sorts the output in each bucket by the given columns on the file system.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      col : str, tuple or list\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n |  \n |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the DataFrame in a text file at the specified path.\n |      The text files will be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      The DataFrame must have only one column that is of string type.\n |      Each row becomes a new line in the output file.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a text file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a text file\n |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n |      ...     df.write.mode(\"overwrite\").text(d)\n |      ...\n |      ...     # Read the text file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n |      +---------+\n |      |alphabets|\n |      +---------+\n |      |        a|\n |      |        b|\n |      |        c|\n |      +---------+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e1a906-e60c-4f5a-a5e3-a560af1b887c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrame in module pyspark.sql.dataframe:\n\nclass DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n |  \n |  A distributed collection of data grouped into named columns.\n |  \n |  .. versionadded:: 1.3.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Examples\n |  --------\n |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n |  and can be created using various functions in :class:`SparkSession`:\n |  \n |  >>> people = spark.createDataFrame([\n |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n |  ... ])\n |  \n |  Once created, it can be manipulated using the various domain-specific-language\n |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n |  \n |  To select a column from the :class:`DataFrame`, use the apply method:\n |  \n |  >>> age_col = people.age\n |  \n |  A more concrete example:\n |  \n |  >>> # To create DataFrame using SparkSession\n |  ... department = spark.createDataFrame([\n |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n |  ...     {\"id\": 2, \"name\": \"ML\"},\n |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n |  ... ])\n |  \n |  >>> people.filter(people.age > 30).join(\n |  ...     department, people.deptId == department.id).groupBy(\n |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n |  +-------+------+-----------+--------+\n |  |   name|gender|avg(salary)|max(age)|\n |  +-------+------+-----------+--------+\n |  |     ML|     F|      150.0|      60|\n |  |PySpark|     M|       75.0|      50|\n |  +-------+------+-----------+--------+\n |  \n |  Notes\n |  -----\n |  A DataFrame should only be created as described above. It should not be directly\n |  created via using the constructor.\n |  \n |  Method resolution order:\n |      DataFrame\n |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n |      pyspark.sql.pandas.conversion.PandasConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n |      Returns the :class:`Column` denoted by ``name``.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Column name to return as :class:`Column`.\n |      \n |      Returns\n |      -------\n |      :class:`Column`\n |          Requested column.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([\n |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      \n |      Retrieve a column instance.\n |      \n |      >>> df.select(df.age).show()\n |      +---+\n |      |age|\n |      +---+\n |      |  2|\n |      |  5|\n |      +---+\n |  \n |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n |      Returns the column as a :class:`Column`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([\n |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      \n |      Retrieve a column instance.\n |      \n |      >>> df.select(df['age']).show()\n |      +---+\n |      |age|\n |      +---+\n |      |  2|\n |      |  5|\n |      +---+\n |      \n |      Select multiple string columns as index.\n |      \n |      >>> df[[\"name\", \"age\"]].show()\n |      +-----+---+\n |      | name|age|\n |      +-----+---+\n |      |Alice|  2|\n |      |  Bob|  5|\n |      +-----+---+\n |      >>> df[df.age > 3].show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |  5| Bob|\n |      +---+----+\n |      >>> df[df[0] > 3].show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |  5| Bob|\n |      +---+----+\n |  \n |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n |      Aggregate on the entire :class:`DataFrame` without groups\n |      (shorthand for ``df.groupBy().agg()``).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      exprs : :class:`Column` or dict of key and value strings\n |          Columns or expressions to aggregate DataFrame by.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Aggregated DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import functions as F\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.agg({\"age\": \"max\"}).show()\n |      +--------+\n |      |max(age)|\n |      +--------+\n |      |       5|\n |      +--------+\n |      >>> df.agg(F.min(df.age)).show()\n |      +--------+\n |      |min(age)|\n |      +--------+\n |      |       2|\n |      +--------+\n |  \n |  alias(self, alias: str) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` with an alias set.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      alias : str\n |          an alias name to be set for the :class:`DataFrame`.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Aliased DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import col, desc\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df_as1 = df.alias(\"df_as1\")\n |      >>> df_as2 = df.alias(\"df_as2\")\n |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n |      >>> joined_df.select(\n |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n |      +-----+-----+---+\n |      | name| name|age|\n |      +-----+-----+---+\n |      |  Tom|  Tom| 14|\n |      |  Bob|  Bob| 16|\n |      |Alice|Alice| 23|\n |      +-----+-----+---+\n |  \n |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n |      Calculates the approximate quantiles of numerical columns of a\n |      :class:`DataFrame`.\n |      \n |      The result of this algorithm has the following deterministic bound:\n |      If the :class:`DataFrame` has N elements and if we request the quantile at\n |      probability `p` up to error `err`, then the algorithm will return\n |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n |      close to (p * N). More precisely,\n |      \n |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n |      \n |      This method implements a variation of the Greenwald-Khanna\n |      algorithm (with some speed optimizations). The algorithm was first\n |      present in [[https://doi.org/10.1145/375663.375670\n |      Space-efficient Online Computation of Quantile Summaries]]\n |      by Greenwald and Khanna.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      col: str, tuple or list\n |          Can be a single column name, or a list of names for multiple columns.\n |      \n |          .. versionchanged:: 2.2.0\n |             Added support for multiple columns.\n |      probabilities : list or tuple\n |          a list of quantile probabilities\n |          Each number must belong to [0, 1].\n |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n |      relativeError : float\n |          The relative target precision to achieve\n |          (>= 0). If set to zero, the exact quantiles are computed, which\n |          could be very expensive. Note that values greater than 1 are\n |          accepted but gives the same result as 1.\n |      \n |      Returns\n |      -------\n |      list\n |          the approximate quantiles at the given probabilities.\n |      \n |          * If the input `col` is a string, the output is a list of floats.\n |      \n |          * If the input `col` is a list or tuple of strings, the output is also a\n |              list, but each element in it is a list of floats, i.e., the output\n |              is a list of list of floats.\n |      \n |      Notes\n |      -----\n |      Null values will be ignored in numerical columns before calculation.\n |      For columns only containing null values, an empty list is returned.\n |  \n |  cache(self) -> 'DataFrame'\n |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Cached DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(1)\n |      >>> df.cache()\n |      DataFrame[id: bigint]\n |  \n |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eager : bool, optional, default True\n |          Whether to checkpoint this :class:`DataFrame` immediately.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Checkpointed DataFrame.\n |      \n |      Notes\n |      -----\n |      This API is experimental.\n |      \n |      Examples\n |      --------\n |      >>> import tempfile\n |      >>> df = spark.createDataFrame([\n |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n |      ...     df.checkpoint(False)\n |      DataFrame[age: bigint, name: string]\n |  \n |  coalesce(self, numPartitions: int) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n |      \n |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n |      there will not be a shuffle, instead each of the 100 new partitions will\n |      claim 10 of the current partitions. If a larger number of partitions is requested,\n |      it will stay at the current number of partitions.\n |      \n |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n |      this may result in your computation taking place on fewer nodes than\n |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n |      you can call repartition(). This will add a shuffle step, but means the\n |      current upstream partitions will be executed in parallel (per whatever\n |      the current partitioning is).\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int\n |          specify the target number of partitions\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(10)\n |      >>> df.coalesce(1).rdd.getNumPartitions()\n |      1\n |  \n |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n |      Selects column based on the column name specified as a regex and returns it\n |      as :class:`Column`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, column name specified as a regex.\n |      \n |      Returns\n |      -------\n |      :class:`Column`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n |      +----+\n |      |Col2|\n |      +----+\n |      |   1|\n |      |   2|\n |      |   3|\n |      +----+\n |  \n |  collect(self) -> List[pyspark.sql.types.Row]\n |      Returns all the records as a list of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      list\n |          List of rows.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df.collect()\n |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n |  \n |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n |      Currently only supports the Pearson Correlation Coefficient.\n |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |      method : str, optional\n |          The correlation method. Currently only supports \"pearson\"\n |      \n |      Returns\n |      -------\n |      float\n |          Pearson Correlation Coefficient of two columns.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n |      >>> df.corr(\"c1\", \"c2\")\n |      -0.3592106040535498\n |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n |      >>> df.corr(\"small\", \"bigger\")\n |      1.0\n |  \n |  count(self) -> int\n |      Returns the number of rows in this :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      int\n |          Number of rows.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      \n |      Return the number of rows in the :class:`DataFrame`.\n |      \n |      >>> df.count()\n |      3\n |  \n |  cov(self, col1: str, col2: str) -> float\n |      Calculate the sample covariance for the given columns, specified by their names, as a\n |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |      \n |      Returns\n |      -------\n |      float\n |          Covariance of two columns.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n |      >>> df.cov(\"c1\", \"c2\")\n |      -18.0\n |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n |      >>> df.cov(\"small\", \"bigger\")\n |      1.0\n |  \n |  createGlobalTempView(self, name: str) -> None\n |      Creates a global temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Name of the view.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      Examples\n |      --------\n |      Create a global temporary view.\n |      \n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.createGlobalTempView(\"people\")\n |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      \n |      Throws an exception if the global temporary view already exists.\n |      \n |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL, +SKIP\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: \"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |      True\n |  \n |  createOrReplaceGlobalTempView(self, name: str) -> None\n |      Creates or replaces a global temporary view using the given name.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      \n |      .. versionadded:: 2.2.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Name of the view.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      Examples\n |      --------\n |      Create a global temporary view.\n |      \n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.createOrReplaceGlobalTempView(\"people\")\n |      \n |      Replace the global temporary view.\n |      \n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |      True\n |  \n |  createOrReplaceTempView(self, name: str) -> None\n |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Name of the view.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      Examples\n |      --------\n |      Create a local temporary view named 'people'.\n |      \n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.createOrReplaceTempView(\"people\")\n |      \n |      Replace the local temporary view.\n |      \n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceTempView(\"people\")\n |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropTempView(\"people\")\n |      True\n |  \n |  createTempView(self, name: str) -> None\n |      Creates a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          Name of the view.\n |      \n |      Examples\n |      --------\n |      Create a local temporary view.\n |      \n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.createTempView(\"people\")\n |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      \n |      Throw an exception if the table already exists.\n |      \n |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL, +SKIP\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: \"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropTempView(\"people\")\n |      True\n |  \n |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n |      Returns the cartesian product with another :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Right side of the cartesian product.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Joined DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df2 = spark.createDataFrame(\n |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n |      +---+-----+------+\n |      |age| name|height|\n |      +---+-----+------+\n |      | 14|  Tom|    80|\n |      | 14|  Tom|    85|\n |      | 23|Alice|    80|\n |      | 23|Alice|    85|\n |      | 16|  Bob|    80|\n |      | 16|  Bob|    85|\n |      +---+-----+------+\n |  \n |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n |      table.\n |      The first column of each row will be the distinct values of `col1` and the column names\n |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n |      Pairs that have no occurrences will have zero as their counts.\n |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column. Distinct items will make the first item of\n |          each row.\n |      col2 : str\n |          The name of the second column. Distinct items will make the column names\n |          of the :class:`DataFrame`.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Frequency matrix of two columns.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n |      +-----+---+---+---+\n |      |c1_c2| 10| 11|  8|\n |      +-----+---+---+---+\n |      |    1|  0|  2|  0|\n |      |    3|  1|  0|  0|\n |      |    4|  0|  0|  2|\n |      +-----+---+---+---+\n |  \n |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n |      the specified columns, so we can run aggregations on them.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      cols : list, str or :class:`Column`\n |          columns to create cube by.\n |          Each element should be a column name (string) or an expression (:class:`Column`)\n |          or list of them.\n |      \n |      Returns\n |      -------\n |      :class:`GroupedData`\n |          Cube of the data by given columns.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n |      +-----+----+-----+\n |      | name| age|count|\n |      +-----+----+-----+\n |      | null|null|    2|\n |      | null|   2|    1|\n |      | null|   5|    1|\n |      |Alice|null|    1|\n |      |Alice|   2|    1|\n |      |  Bob|null|    1|\n |      |  Bob|   5|    1|\n |      +-----+----+-----+\n |  \n |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n |      Computes basic statistics for numeric and string columns.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      This incl\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ninput_df):\n |      ...     return input_df.select(*sorted(input_df.columns))\n |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n |      +-----+---+\n |      |float|int|\n |      +-----+---+\n |      |    1|  1|\n |      |    2|  2|\n |      +-----+---+\n |      \n |      >>> def add_n(input_df, n):\n |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n |      ...                             for col_name in input_df.columns])\n |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n |      +---+-----+\n |      |int|float|\n |      +---+-----+\n |      | 12| 12.0|\n |      | 13| 13.0|\n |      +---+-----+\n |  \n |  union(self, other: 'DataFrame') -> 'DataFrame'\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Another :class:`DataFrame` that needs to be unioned\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      See Also\n |      --------\n |      DataFrame.unionAll\n |      \n |      Notes\n |      -----\n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      Examples\n |      --------\n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n |      >>> df1.union(df2).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   4|   5|   6|\n |      +----+----+----+\n |      >>> df1.union(df1).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   1|   2|   3|\n |      +----+----+----+\n |  \n |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Another :class:`DataFrame` that needs to be combined\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Combined DataFrame\n |      \n |      Notes\n |      -----\n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      :func:`unionAll` is an alias to :func:`union`\n |      \n |      See Also\n |      --------\n |      DataFrame.union\n |  \n |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Another :class:`DataFrame` that needs to be combined.\n |      allowMissingColumns : bool, optional, default False\n |         Specify whether to allow missing columns.\n |      \n |         .. versionadded:: 3.1.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Combined DataFrame.\n |      \n |      Examples\n |      --------\n |      The difference between this function and :func:`union` is that this function\n |      resolves columns by name (not by position):\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n |      >>> df1.unionByName(df2).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   6|   4|   5|\n |      +----+----+----+\n |      \n |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n |      in the schema of the union result:\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n |      +----+----+----+----+\n |      |col0|col1|col2|col3|\n |      +----+----+----+----+\n |      |   1|   2|   3|null|\n |      |null|   4|   5|   6|\n |      +----+----+----+----+\n |  \n |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n |      \n |      Parameters\n |      ----------\n |      blocking : bool\n |          Whether to block until all blocks are deleted.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Unpersisted DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(1)\n |      >>> df.persist()\n |      DataFrame[id: bigint]\n |      >>> df.unpersist()\n |      DataFrame[id: bigint]\n |      >>> df = spark.range(1)\n |      >>> df.unpersist(True)\n |      DataFrame[id: bigint]\n |  \n |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n |      Unpivot a DataFrame from wide format to long format, optionally leaving\n |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n |      except for the aggregation, which cannot be reversed.\n |      \n |      This function is useful to massage a DataFrame into a format where some\n |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n |      by `variableColumnName` and `valueColumnName`.\n |      \n |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n |      \"variable\" and \"value\" columns.\n |      \n |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n |      When `values` is `None`, all non-id columns will be unpivoted.\n |      \n |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n |      all \"value\" columns are cast to the nearest common data type. For instance, types\n |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n |      do not have a common data type and `unpivot` fails.\n |      \n |      .. versionadded:: 3.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      ids : str, Column, tuple, list\n |          Column(s) to use as identifiers. Can be a single column or column name,\n |          or a list or tuple for multiple columns.\n |      values : str, Column, tuple, list, optional\n |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n |          for multiple columns. If specified, must not be empty. If not specified, uses all\n |          columns that are not set as `ids`.\n |      variableColumnName : str\n |          Name of the variable column.\n |      valueColumnName : str\n |          Name of the value column.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Unpivoted DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n |      ...     [\"id\", \"int\", \"double\"],\n |      ... )\n |      >>> df.show()\n |      +---+---+------+\n |      | id|int|double|\n |      +---+---+------+\n |      |  1| 11|   1.1|\n |      |  2| 12|   1.2|\n |      +---+---+------+\n |      \n |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n |      +---+------+----+\n |      | id|   var| val|\n |      +---+------+----+\n |      |  1|   int|11.0|\n |      |  1|double| 1.1|\n |      |  2|   int|12.0|\n |      |  2|double| 1.2|\n |      +---+------+----+\n |      \n |      See Also\n |      --------\n |      DataFrame.melt\n |  \n |  where = filter(self, condition)\n |      :func:`where` is an alias for :func:`filter`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` by adding a column or replacing the\n |      existing column that has the same name.\n |      \n |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n |      a column from some other :class:`DataFrame` will raise an error.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, name of the new column.\n |      col : :class:`Column`\n |          a :class:`Column` expression for the new column.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with new or replaced column.\n |      \n |      Notes\n |      -----\n |      This method introduces a projection internally. Therefore, calling it multiple\n |      times, for instance, via loops in order to add multiple columns can generate big\n |      plans which can cause performance issues and even `StackOverflowException`.\n |      To avoid this, use :func:`select` with multiple columns at once.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.withColumn('age2', df.age + 2).show()\n |      +---+-----+----+\n |      |age| name|age2|\n |      +---+-----+----+\n |      |  2|Alice|   4|\n |      |  5|  Bob|   7|\n |      +---+-----+----+\n |  \n |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` by renaming an existing column.\n |      This is a no-op if the schema doesn't contain the given column name.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      existing : str\n |          string, name of the existing column to rename.\n |      new : str\n |          string, new name of the column.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with renamed column.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.withColumnRenamed('age', 'age2').show()\n |      +----+-----+\n |      |age2| name|\n |      +----+-----+\n |      |   2|Alice|\n |      |   5|  Bob|\n |      +----+-----+\n |  \n |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n |      existing columns that have the same names.\n |      \n |      The colsMap is a map of column name and column, the column must only refer to attributes\n |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n |      \n |      .. versionadded:: 3.3.0\n |         Added support for multiple columns adding\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      colsMap : dict\n |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with new or replaced columns.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n |      +---+-----+----+----+\n |      |age| name|age2|age3|\n |      +---+-----+----+----+\n |      |  2|Alice|   4|   5|\n |      |  5|  Bob|   7|   8|\n |      +---+-----+----+----+\n |  \n |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      columnName : str\n |          string, name of the existing column to update the metadata.\n |      metadata : dict\n |          dict, new metadata to be assigned to df.schema[columnName].metadata\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with updated metadata column.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n |      >>> df_meta.schema['age'].metadata\n |      {'foo': 'bar'}\n |  \n |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n |      in time before which we assume no more late data is going to arrive.\n |      \n |      Spark will use this watermark for several purposes:\n |        - To know when a given time window aggregation can be finalized and thus can be emitted\n |          when using output modes that do not allow updates.\n |      \n |        - To minimize the amount of state that we need to keep for on-going aggregations.\n |      \n |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n |      process records that arrive more than `delayThreshold` late.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eventTime : str\n |          the name of the column that contains the event time of the row.\n |      delayThreshold : str\n |          the minimum delay to wait to data to arrive late, relative to the\n |          latest record that has been processed in the form of an interval\n |          (e.g. \"1 minute\" or \"5 hours\").\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          Watermarked DataFrame\n |      \n |      Notes\n |      -----\n |      This is a feature only for Structured Streaming.\n |      \n |      This API is evolving.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> from pyspark.sql.functions import timestamp_seconds\n |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n |      ...     \"value % 5 AS value\", \"timestamp\")\n |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n |      DataFrame[value: bigint, time: timestamp]\n |      \n |      Group the data by window and value (0 - 4), and compute the count of each group.\n |      \n |      >>> import time\n |      >>> from pyspark.sql.functions import window\n |      >>> query = (df\n |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n |      ...     .groupBy(\n |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n |      ...         df.value)\n |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n |      >>> time.sleep(3)\n |      >>> query.stop()\n |  \n |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n |      Create a write configuration builder for v2 sources.\n |      \n |      This builder is used to configure and execute write operations.\n |      \n |      For example, to append or create or replace existing tables.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Target table name to write to.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameWriterV2`\n |          DataFrameWriterV2 to use further to specify how to save the data\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n |      >>> df.writeTo(                              # doctest: +SKIP\n |      ...     \"catalog.db.table\"\n |      ... ).partitionedBy(\"col\").createOrReplace()\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  columns\n |      Returns all column names as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      list\n |          List of column names.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df.columns\n |      ['age', 'name']\n |  \n |  dtypes\n |      Returns all column names and their data types as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      list\n |          List of columns as tuple pairs.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      >>> df.dtypes\n |      [('age', 'bigint'), ('name', 'string')]\n |  \n |  isStreaming\n |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n |      is a streaming source present.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      bool\n |          Whether it's streaming DataFrame or not.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.readStream.format(\"rate\").load()\n |      >>> df.isStreaming\n |      True\n |  \n |  na\n |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameNaFunctions`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n |      >>> type(df.na)\n |      <class '...dataframe.DataFrameNaFunctions'>\n |      \n |      Replace the missing values as 2.\n |      \n |      >>> df.na.fill(2).show()\n |      +---+---+\n |      | c1| c2|\n |      +---+---+\n |      |  1|  2|\n |      +---+---+\n |  \n |  rdd\n |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(1)\n |      >>> type(df.rdd)\n |      <class 'pyspark.rdd.RDD'>\n |  \n |  schema\n |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame(\n |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n |      \n |      Retrieve the schema of the current DataFrame.\n |      \n |      >>> df.schema\n |      StructType([StructField('age', LongType(), True),\n |                  StructField('name', StringType(), True)])\n |  \n |  sparkSession\n |      Returns Spark session that created this :class:`DataFrame`.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      :class:`SparkSession`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(1)\n |      >>> type(df.sparkSession)\n |      <class '...session.SparkSession'>\n |  \n |  sql_ctx\n |  \n |  stat\n |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameStatFunctions`\n |      \n |      Examples\n |      --------\n |      >>> import pyspark.sql.functions as f\n |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n |      >>> type(df.stat)\n |      <class '...dataframe.DataFrameStatFunctions'>\n |      >>> df.stat.corr(\"id\", \"c\")\n |      1.0\n |  \n |  storageLevel\n |      Get the :class:`DataFrame`'s current storage level.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Returns\n |      -------\n |      :class:`StorageLevel`\n |          Currently defined storage level.\n |      \n |      Examples\n |      --------\n |      >>> df1 = spark.range(10)\n |      >>> df1.storageLevel\n |      StorageLevel(False, False, False, False, 1)\n |      >>> df1.cache().storageLevel\n |      StorageLevel(True, True, False, True, 1)\n |      \n |      >>> df2 = spark.range(5)\n |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n |      StorageLevel(True, False, False, False, 2)\n |  \n |  write\n |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameWriter`\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n |      >>> type(df.write)\n |      <class '...readwriter.DataFrameWriter'>\n |      \n |      Write the DataFrame as a table.\n |      \n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n |      >>> df.write.saveAsTable(\"tab2\")\n |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n |  \n |  writeStream\n |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`DataStreamWriter`\n |      \n |      Examples\n |      --------\n |      >>> import tempfile\n |      >>> df = spark.readStream.format(\"rate\").load()\n |      >>> type(df.writeStream)\n |      <class 'pyspark.sql.streaming.readwriter.DataStreamWriter'>\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Create a table with Rate source.\n |      ...     df.writeStream.toTable(\n |      ...         \"my_table\", checkpointLocation=d) # doctest: +ELLIPSIS\n |      <pyspark.sql.streaming.query.StreamingQuery object at 0x...>\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fdf677-4ae2-4d54-844f-31114dcd98f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| Id|  Name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1,'Maheer'),(2, 'wafa')]\n",
    "\n",
    "schema = StructType([StructField(name = 'Id', dataType = IntegerType()),\n",
    "            StructField(name = 'Name', dataType = StringType())])\n",
    "\n",
    "type(schema)\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.write.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484f8049-698b-451d-8098-ad13a268f0db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| Id|  Name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Maheer</td></tr><tr><td>2</td><td>wafa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Maheer"
        ],
        [
         "2",
         "wafa"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True, mode = 'overwrite')\n",
    "\n",
    "df.show()\n",
    "\n",
    "display(spark.read.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f8b0ed-6803-4c40-bc76-9c138d2803bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| Id|  Name|\n+---+------+\n|  1|Maheer|\n|  2|  wafa|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True, mode = 'append')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2a2cad-4435-4940-a45a-eacc78c37086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Maheer</td></tr><tr><td>1</td><td>Maheer</td></tr><tr><td>1</td><td>Maheer</td></tr><tr><td>2</td><td>wafa</td></tr><tr><td>2</td><td>wafa</td></tr><tr><td>2</td><td>wafa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Maheer"
        ],
        [
         "1",
         "Maheer"
        ],
        [
         "1",
         "Maheer"
        ],
        [
         "2",
         "wafa"
        ],
        [
         "2",
         "wafa"
        ],
        [
         "2",
         "wafa"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f452e0c1-8b7c-4034-87ed-182555151b0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Maheer</td></tr><tr><td>2</td><td>wafa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Maheer"
        ],
        [
         "2",
         "wafa"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True, mode = 'ignore')\n",
    "\n",
    "display(spark.read.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbccd6a-0b2f-4fba-b211-93115174d14e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3409072812134660>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dataset2\u001B[39m\u001B[38;5;124m'\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m display(spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dataset2\u001B[39m\u001B[38;5;124m'\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/dataset2 already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3409072812134660>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dataset2\u001B[39m\u001B[38;5;124m'\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m display(spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dataset2\u001B[39m\u001B[38;5;124m'\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/dataset2 already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/tables/dataset2 already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True, mode = 'error')\n",
    "\n",
    "display(spark.read.csv(path = 'dbfs:/FileStore/tables/dataset2', header = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9f4c99-3cec-40db-b6c1-8b0bdf8ff506",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## error above is expected "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "csv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
