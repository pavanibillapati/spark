{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af45dd66-2bb8-4444-8d4a-592091c4b474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n|  3|   asi| 18000|     F|\n|  2|mahesh| 24000|     M|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 20000, 'M'), (2, 'mahesh', 24000, 'M'), (3, 'asi', 18000, 'F'), (2, 'mahesh', 24000, 'M')]\n",
    "schema = ('id', 'name', 'salary', 'gender')\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb4d7fe-535b-4643-871b-90feecf2aca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n\ndistinct() -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with distinct records.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n    \n    Return the number of distinct rows in the :class:`DataFrame`\n    \n    >>> df.distinct().count()\n    2\n\n"
     ]
    }
   ],
   "source": [
    "help(df.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666a8890-7723-44aa-a9e6-fb4584646a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n|  3|   asi| 18000|     F|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be2d65c-5629-495f-a31c-dfd6c4caec3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n|  3|   asi| 18000|     F|\n|  2|mahesh| 24000|     M|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40dfbe55-6b28-4e0c-878c-314d884c7cde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n|  3|   asi| 18000|     F|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52a80f9-a303-458c-a21a-00a79d647a27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n|  3|   asi| 18000|     F|\n|  2|mahesh| 24000|     M|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b2bd06-caae-4127-aec9-ef9307029bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n\ndropDuplicates(subset: Optional[List[str]] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Return a new :class:`DataFrame` with duplicate rows removed,\n    optionally only considering certain columns.\n    \n    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n    be and the system will accordingly limit the state. In addition, data older than\n    watermark will be dropped to avoid any possibility of duplicates.\n    \n    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    subset : List of column names, optional\n        List of columns to use for duplicate comparison (default All columns).\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame without duplicates.\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> df = spark.createDataFrame([\n    ...     Row(name='Alice', age=5, height=80),\n    ...     Row(name='Alice', age=5, height=80),\n    ...     Row(name='Alice', age=10, height=80)\n    ... ])\n    \n    Deduplicate the same rows.\n    \n    >>> df.dropDuplicates().show()\n    +-----+---+------+\n    | name|age|height|\n    +-----+---+------+\n    |Alice|  5|    80|\n    |Alice| 10|    80|\n    +-----+---+------+\n    \n    Deduplicate values on 'name' and 'height' columns.\n    \n    >>> df.dropDuplicates(['name', 'height']).show()\n    +-----+---+------+\n    | name|age|height|\n    +-----+---+------+\n    |Alice|  5|    80|\n    +-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "help(df.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4523d6-b708-4056-9318-309b74e6b3a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  3|   asi| 18000|     F|\n|  1|maheer| 20000|     M|\n|  2|mahesh| 24000|     M|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['gender', 'salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7893fa9-92d4-477b-a030-6012f0056852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|salary|gender|\n+---+------+------+------+\n|  3|   asi| 18000|     F|\n|  1|maheer| 20000|     M|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af45096-c279-4f89-9cd3-f394ddb5ad41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "distinct_drop duplicates",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
